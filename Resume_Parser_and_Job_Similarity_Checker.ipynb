{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a846d4b1"
      },
      "source": [
        "First, we need to install the `transformers` library, which provides pre-trained BERT models and tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7367b0b",
        "outputId": "afaa1947-7e75-4e74-ba51-013abf2ec092"
      },
      "source": [
        "pip install transformers torch"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20c4d2db"
      },
      "source": [
        "Now, let's import the necessary modules from the `transformers` library and define the BERT model and tokenizer we'll use (e.g., `bert-base-uncased`). We'll also define some example sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "342c2eea"
      },
      "source": [
        "def load_bert():\n",
        "  from transformers import BertModel, BertTokenizer\n",
        "\n",
        "  # Load pre-trained BERT model and tokenizer\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "  # Example sentences\n",
        "  sentences = [\n",
        "      \"This is a sample sentence.\",\n",
        "      \"Hello, how are you today?\",\n",
        "      \"BERT is a powerful language model.\"\n",
        "  ]\n",
        "\n",
        "  print(\"BERT model and tokenizer loaded successfully.\")\n",
        "  return tokenizer, model"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3253b2c8"
      },
      "source": [
        "Next, we will tokenize the sentences. Tokenization converts the sentences into numerical inputs that the BERT model can understand. We'll also ensure that all sentences are padded to the same length and attention masks are created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a1d9778"
      },
      "source": [
        "def bert_processing(sentence):\n",
        "  # Tokenize sentences\n",
        "  tokenizer, model = load_bert()\n",
        "  encoded_input = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "  print(\"Tokenized input:\")\n",
        "  for key, value in encoded_input.items():\n",
        "      print(f\"{key}: {value.shape}\")\n",
        "      # print(value)\n",
        "  return encoded_input, model\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92969ef3"
      },
      "source": [
        "Finally, we pass the tokenized inputs through the BERT model to obtain the embeddings. We'll extract the embeddings corresponding to the `[CLS]` token, which are often used as sentence-level representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7249ec0c"
      },
      "source": [
        "def bert_generated_embeddings(sentence):\n",
        "  import torch\n",
        "  encoded_input, model = bert_processing(sentence)\n",
        "  # Get model outputs\n",
        "  with torch.no_grad(): # Disable gradient calculation for inference\n",
        "      model_output = model(**encoded_input)\n",
        "\n",
        "  # The last_hidden_state contains the embeddings for all tokens\n",
        "  # The pooler_output (optional) can be used, but [CLS] token is common for sentence embeddings\n",
        "  # To get the [CLS] token embedding, we take the first token's embedding from the last_hidden_state\n",
        "  sentence_embeddings = model_output.last_hidden_state[:, 0, :]\n",
        "  return sentence_embeddings\n",
        "  # print(\"Shape of sentence embeddings (number_of_sentences, embedding_dimension):\")\n",
        "  # print(sentence_embeddings.shape)\n",
        "  # print(\"\\nFirst sentence embedding (first 10 dimensions):\\n\", sentence_embeddings[0, :10])\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57f439d5"
      },
      "source": [
        "First, we'll install the necessary libraries: `tensorflow` (which Keras is now a part of) and `tensorflow-text` for the BERT preprocessor."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxhC2HwCtltv",
        "outputId": "d89b9af0-d465-4aaa-807c-5579dea3875b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.12/dist-packages (1.26.6)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.121.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.4)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.3)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.49.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatizing the text\n",
        "# Importing the required libraries\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "nin-ZjtxvIwu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required resources\n",
        "nltk.download('wordnet')                 # WordNet dictionary\n",
        "nltk.download('omw-1.4')                 # Multilingual WordNet support\n",
        "nltk.download('averaged_perceptron_tagger')  # POS tagger\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCSNocbYvNDz",
        "outputId": "125d3437-d608-450e-cbcc-1e8318e4601d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuations(sentence):\n",
        "    import re\n",
        "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "YEjq8klqvPKk"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(sentence):\n",
        "  sentence = sentence.lower()\n",
        "  sentence = remove_punctuations(sentence)\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "jMO8PUxMvTPl"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# When passed a word it will give it's POS using wordnet\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "metadata": {
        "id": "vqRJ_-YPvU2y"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatizing the sentence\n",
        "def lemmatize_sentence(sentence):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    sentence = preprocess(sentence)\n",
        "    res = []\n",
        "    sen = nltk.sent_tokenize(sentence)\n",
        "    for j in sen:\n",
        "        j = remove_punctuations(j)\n",
        "        words = nltk.word_tokenize(j)\n",
        "        lemmatized = [lemmatizer.lemmatize(w, pos=get_wordnet_pos(w)) for w in words]\n",
        "        res.extend(lemmatized)\n",
        "    return res"
      ],
      "metadata": {
        "id": "mCOtu8-hvYTU"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_1 = \"Proficient in Injury Prevention, Motivation, Nutrition, Health Coaching, Strength Training, with mid-level experience in the field. Holds a Bachelors degree. Holds certifications such as Certified Personal Trainer (CPT) by NASM. Skilled in delivering results and adapting to dynamic environments.\"\n",
        "sentence_2 = \" A Fitness Coach is responsible for helping clients achieve their fitness goals by designing and leading group or individual fitness programs. You will provide instruction on exercises, proper form, and injury prevention techniques, encouraging clients to push their limits while maintaining a focus on their well-being. The role requires a passion for health and fitness, a strong understanding of exercise physiology, and the ability to motivate and inspire others. You will also monitor clients’ progress and make adjustments to their fitness plans as needed to ensure continuous improvement.\""
      ],
      "metadata": {
        "id": "KtdDy6pxvaZr"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_1 = lemmatize_sentence(sentence_1)\n",
        "sentence_2 = lemmatize_sentence(sentence_2)"
      ],
      "metadata": {
        "id": "qdjYMWy5vcN5"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data(resume_data):\n",
        "  import re\n",
        "  pattern = r\"Technical Skills(.*?)Extracurricular Activities\"\n",
        "  matches = re.search(pattern, resume_data, re.DOTALL)\n",
        "\n",
        "  if matches:\n",
        "      technical_skills = matches.group(1).strip()\n",
        "      # print(\"### Technical Skills ###\")\n",
        "      # print(technical_skills)\n",
        "  else:\n",
        "      print(\"Technical Skills section not found.\")\n",
        "\n",
        "  skills_list = re.split(r'\\n(?=•)', technical_skills.strip())\n",
        "\n",
        "  cleaned_text = [re.sub(r'\\s+', ' ', item).replace('•', '').strip() for item in skills_list]\n",
        "\n",
        "  result_dict = {}\n",
        "  for item in cleaned_text:\n",
        "      key, value = item.split(':', 1)\n",
        "      result_dict[key.strip()] = value.strip()\n",
        "\n",
        "  print(result_dict)\n",
        "\n",
        "  final_text = \"Proficient in \" + result_dict['Programming Languages'] + \". Knowns Web Technologies which includes \" + result_dict[\"Web Technologies\"] + \". Familiar with \" + result_dict[\"Engineering Software\"]\n",
        "  return final_text"
      ],
      "metadata": {
        "id": "uykaanSDveAH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_score( sentence_1, sentence_2):\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "  # Compute cosine similarity\n",
        "  similarity = cosine_similarity(sentence_1, sentence_2)\n",
        "  return similarity"
      ],
      "metadata": {
        "id": "syBynH9b1rNW"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_score(sentence_1, sentence_2):\n",
        "\n",
        "  check = sentence_2[-1]\n",
        "\n",
        "  sentence_1 = lemmatize_sentence(sentence_1)\n",
        "  sentence_2 = lemmatize_sentence(sentence_2)\n",
        "\n",
        "  emb_1 = bert_generated_embeddings(sentence_1)\n",
        "  emb_2 = bert_generated_embeddings(sentence_2)\n",
        "\n",
        "  # resume_emb_stack_1 = stack_tensors_1(emb_1, len(emb_1), emb_1[0].shape[0])\n",
        "  # job_emb_stack_1 = stack_tensors_1(emb_2, len(emb_2), emb_2[0].shape[0])\n",
        "\n",
        "  score = get_score(emb_1, emb_2)\n",
        "  return score"
      ],
      "metadata": {
        "id": "iYqgbSWUywt6"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# Function to read the PDF file using PyMuPDF\n",
        "def read_pdf(file):\n",
        "    try:\n",
        "        # Open the uploaded PDF file using PyMuPDF\n",
        "        doc = fitz.open(file.name)\n",
        "        resume_data = \"\"\n",
        "\n",
        "        # Extract resume_data from all pages in the PDF\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)  # Load each page\n",
        "            resume_data += page.get_text()  # Extract resume_data from the page\n",
        "\n",
        "        sentence_1 = extract_data(resume_data)\n",
        "        sentence_2 = 'Requred a software developer who is proficient in C, Java, Python, JavaScript, PHP and related languages. Knowns Web Technologies which includes HTML, CSS, Django. Familiar with Visual Studio, GitHub, PyCharm, IntelliJ, MySQL, GNU 8085 Simulator.T'\n",
        "        # sentence_2 = 'As a Personal Trainer, you will design personalized fitness programs that help clients achieve their physical health goals. Your role involves motivating clients to push their limits, providing expert advice on exercise techniques, and offering nutritional guidance. You will work with individuals at different fitness levels, providing support and encouragement to help them improve their strength, endurance, and overall well-being. The role demands a passion for fitness, excellent interpersonal skills, and the ability to inspire others. You will also stay up-to-date with the latest trends in health and fitness to ensure that your training methods remain effective and innovative.F'\n",
        "\n",
        "        matching_score = similarity_score(sentence_1, sentence_2)\n",
        "        matching = 1 if matching_score[0][0] >= 0.5 else 0\n",
        "\n",
        "        return sentence_1, matching\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error reading PDF: {e}\"\n",
        "\n",
        "# Create a Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=read_pdf,  # Function to process the uploaded PDF\n",
        "    inputs=gr.File(label=\"Upload a PDF File\"),  # File input for uploading PDF\n",
        "    outputs = [\n",
        "        gr.Textbox(label=\"Parsed Text\"),  # Display the extracted text\n",
        "        gr.Textbox(label=\"Similairty\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "4jjkrC-h6CY_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "b091ed1b-f885-446d-b19f-493c47bb5e84"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ef9b4732a097de4c5a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ef9b4732a097de4c5a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    }
  ]
}